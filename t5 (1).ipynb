{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: sentencePiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers sentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1330 Thirukkural entries.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "# Load your Thirukkural dataset\n",
    "dataset_path = \"/teamspace/studios/this_studio/dataset/thirukkural.json\"\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    thirukkural_data = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(thirukkural_data)} Thirukkural entries.\")\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "hf_data = Dataset.from_list([\n",
    "    {\"instruction\": entry[\"instruction\"], \"response\": json.dumps(entry[\"response\"])}\n",
    "    for entry in thirukkural_data\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b7bd144c6c44b8b0e53d2daf9d4eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='999' max='999' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [999/999 04:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.526704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.512214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=999, training_loss=0.7392757137019832, metrics={'train_runtime': 242.4368, 'train_samples_per_second': 16.458, 'train_steps_per_second': 4.121, 'total_flos': 540013787873280.0, 'train_loss': 0.7392757137019832, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load T5 small model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize data\n",
    "def preprocess_data(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"instruction\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        example[\"response\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = hf_data.map(preprocess_data)\n",
    "\n",
    "# Fine-tuning \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# InitTrainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_finetuned/tokenizer_config.json',\n",
       " './t5_finetuned/special_tokens_map.json',\n",
       " './t5_finetuned/spiece.model',\n",
       " './t5_finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save ft model\n",
    "model.save_pretrained(\"./t5_finetuned\")\n",
    "tokenizer.save_pretrained(\"./t5_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "\"Number\": \"3\", \"kural\": \"u0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcd u0ba4u0bc1u0baeu0bcdu0ba9u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcd u0ba4u0bc1u0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcdu0ba4u0bcd\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_finetuned\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5_finetuned\")\n",
    "\n",
    "# Generate a response\n",
    "def generate_response(instruction):\n",
    "    inputs = tokenizer(instruction, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=512, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example question\n",
    "instruction = \"Provide all details about Kural number 3.\"\n",
    "response = generate_response(instruction)\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting indic-nlp-library\n",
      "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (4.47.1)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (2.2.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (0.27.0)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Collecting sphinx-argparse (from indic-nlp-library)\n",
      "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
      "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting morfessor (from indic-nlp-library)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from indic-nlp-library) (1.26.4)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting sphinx>=5.1.0 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinx-8.1.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting docutils>=0.19 (from sphinx-argparse->indic-nlp-library)\n",
      "  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
      "Collecting sphinxcontrib-applehelp>=1.0.7 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath>=1.0.1 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-qthelp>=1.0.6 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: Pygments>=2.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: babel>=2.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.16.0)\n",
      "Collecting alabaster>=0.7.14 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading alabaster-1.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tomli>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
      "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m185.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinx-8.1.3-py3-none-any.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Downloading alabaster-1.0.0-py3-none-any.whl (13 kB)\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Installing collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, docutils, alabaster, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, sentence-transformers, indic-nlp-library\n",
      "Successfully installed alabaster-1.0.0 docutils-0.21.2 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sentence-transformers-3.3.1 snowballstemmer-2.2.0 sphinx-8.1.3 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ai4bharat/indic-bert. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140db9faca7e490a860e8f10c931a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39238787ec64014af90bf5f2ff92b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c26c5d1d0a94bb78ea5822cb62925b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Response:\n",
      "Number: 5\n",
      "Kural: ‡Æá‡Æ∞‡ØÅ‡Æ≥‡Øç‡Æö‡Øá‡Æ∞‡Øç ‡Æá‡Æ∞‡ØÅ‡Æµ‡Æø‡Æ©‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æö‡Øá‡Æ∞‡Ææ ‡Æá‡Æ±‡Øà‡Æµ‡Æ©‡Øç ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç‡Æö‡Øá‡Æ∞‡Øç ‡Æ™‡ØÅ‡Æï‡Æ¥‡Øç‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡Ææ‡Æ∞‡Øç ‡ÆÆ‡Ææ‡Æü‡Øç‡Æü‡ØÅ.\n",
      "Meaning (MK): ‡Æá‡Æ±‡Øà‡Æµ‡Æ©‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡Æ±‡Øç‡Æï‡ØÅ‡Æ∞‡Æø‡ÆØ ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ ‡Æ™‡ØÅ‡Æï‡Æ¥‡Øç ‡Æ™‡ØÜ‡Æ± ‡Æµ‡Æø‡Æ∞‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç, ‡Æ®‡Æ©‡Øç‡ÆÆ‡Øà ‡Æ§‡ØÄ‡ÆÆ‡Øà‡Æï‡Æ≥‡Øà ‡Æí‡Æ∞‡Øá ‡ÆÖ‡Æ≥‡Æµ‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Æø‡Æ∞‡Øç ‡Æï‡Øä‡Æ≥‡Øç‡Æµ‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç\n",
      "Explanation: The two-fold deeds that spring from darkness shall not adhere to those who delight in the true praise of God\n",
      "Adikaram: ‡Æï‡Æü‡Æµ‡ØÅ‡Æ≥‡Øç ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ\n",
      "Iyal: ‡Æ™‡Ææ‡ÆØ‡Æø‡Æ∞‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [responses[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Example of retrieving the top 3 matches\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m top_matches \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_top_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_matches, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36mretrieve_top_matches\u001b[0;34m(question, top_n)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Get top N matches\u001b[39;00m\n\u001b[1;32m     74\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39mtopk(top_n)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [responses[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices]\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Get top N matches\u001b[39;00m\n\u001b[1;32m     74\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39mtopk(top_n)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "# necessary packages \n",
    "!pip install sentence-transformers indic-nlp-library\n",
    "\n",
    "# libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "# Load the Tamil Sentence Transformer model (Indic-BERT)\n",
    "model = SentenceTransformer(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Load the Thirukkural dataset\n",
    "dataset_path = \"/teamspace/studios/this_studio/dataset/thirukkural.json\"\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    thirukkural_data = json.load(file)\n",
    "\n",
    "# Preprocess the dataset to extract instructions and responses\n",
    "instructions = [entry[\"instruction\"] for entry in thirukkural_data]\n",
    "responses = [entry[\"response\"] for entry in thirukkural_data]\n",
    "\n",
    "# Encode the instructions into embeddings\n",
    "instruction_embeddings = model.encode(instructions, convert_to_tensor=True)\n",
    "\n",
    "# normalize Tamil text\n",
    "normalizer = IndicNormalizerFactory().get_normalizer(\"ta\")\n",
    "\n",
    "def normalize_input(text):\n",
    "    \"\"\"\n",
    "    Normalize Tamil text to improve matching accuracy.\n",
    "    \"\"\"\n",
    "    return normalizer.normalize(text)\n",
    "\n",
    "# Define the retrieval function to fetch the best match\n",
    "def retrieve_best_match(question):\n",
    "    \"\"\"\n",
    "    Given a question, retrieve the best matching Kural from the dataset.\n",
    "    \"\"\"\n",
    "    # Normalize the question\n",
    "    normalized_question = normalize_input(question)\n",
    "    \n",
    "    # Encode the question to get its embedding\n",
    "    question_embedding = model.encode(normalized_question, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between the question and all instructions\n",
    "    similarities = util.cos_sim(question_embedding, instruction_embeddings)\n",
    "    \n",
    "    # Find the best match (highest similarity score)\n",
    "    best_match_idx = similarities.argmax().item()\n",
    "    return responses[best_match_idx]\n",
    "\n",
    "# Test the retrieval system\n",
    "question = \"What is the meaning of Kural number 1?\"\n",
    "best_response = retrieve_best_match(question)\n",
    "\n",
    "# Display the retrieved response\n",
    "print(\"Retrieved Response:\")\n",
    "print(f\"Number: {best_response['Number']}\")\n",
    "print(f\"Kural: {best_response['kural']}\")\n",
    "print(f\"Meaning (MK): {best_response['mk']}\")\n",
    "print(f\"Explanation: {best_response['explanation']}\")\n",
    "print(f\"Adikaram: {best_response['adikaram_name']}\")\n",
    "print(f\"Iyal: {best_response['iyal_name']}\")\n",
    "\n",
    "# Optional - Retrieve top N matches for ranking\n",
    "def retrieve_top_matches(question, top_n=3):\n",
    "    \"\"\"\n",
    "    Retrieve the top N best matching Kurals for a given question.\n",
    "    \"\"\"\n",
    "    normalized_question = normalize_input(question)\n",
    "    question_embedding = model.encode(normalized_question, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(question_embedding, instruction_embeddings)\n",
    "    \n",
    "    # Get top N matches\n",
    "    top_indices = similarities.topk(top_n).indices.tolist()\n",
    "    return [responses[idx] for idx in top_indices]\n",
    "\n",
    "# Example matches\n",
    "top_matches = retrieve_top_matches(question, top_n=1)\n",
    "for idx, match in enumerate(top_matches, 1):\n",
    "    print(f\"Match {idx}:\")\n",
    "    print(f\"Number: {match['Number']}\")\n",
    "    print(f\"Kural: {match['kural']}\")\n",
    "    print(f\"Explanation: {match['explanation']}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding when saving the dataset\n",
    "import json\n",
    "\n",
    "with open(\"/teamspace/studios/this_studio/dataset/thirukkural.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(thirukkural_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: ‡ÆÖ‡Æï‡Æ∞ ‡ÆÆ‡ØÅ‡Æ§‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÜ‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç ‡ÆÜ‡Æ§‡Æø ‡Æ™‡Æï‡Æµ‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ±‡Øç‡Æ±‡Øá ‡Æâ‡Æ≤‡Æï‡ØÅ.\n",
      "Tokenized: tensor([[3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 5, 1]])\n",
      "Decoded: <unk> <unk> <unk> <unk> <unk> <unk> <unk>.</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"‡ÆÖ‡Æï‡Æ∞ ‡ÆÆ‡ØÅ‡Æ§‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÜ‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç ‡ÆÜ‡Æ§‡Æø ‡Æ™‡Æï‡Æµ‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ±‡Øç‡Æ±‡Øá ‡Æâ‡Æ≤‡Æï‡ØÅ.\"\n",
    "tokenized = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(tokenized[0])\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokenized:\", tokenized)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=512,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.2,  # Penalize repeated tokens\n",
    "    length_penalty=1.0,      # Prefer natural-length sentences\n",
    "    no_repeat_ngram_size=2   # Avoid repetitive n-grams\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
