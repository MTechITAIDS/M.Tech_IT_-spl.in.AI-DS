{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch transformers datasets pandas numpy sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def convert_csv_to_training_format(csv_file: str) -> List[Dict]:\n",
    "    df = pd.read_csv(\"/teamspace/studios/this_studio/Dataset/thirukkural.csv\")\n",
    "    training_data = []\n",
    "    \n",
    "    # Create different question patterns\n",
    "    question_patterns = [\n",
    "        \"What is Thirukkural {number}?\",\n",
    "        \"Tell me about the Thirukkural from {adikaram_name}\",\n",
    "        \"What does Thirukkural say in {paul_name}?\",\n",
    "        \"Explain the meaning of Thirukkural {number}\",\n",
    "        \"What is the explanation of Thirukkural about {adikaram_name}?\"\n",
    "    ]\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        response = {\n",
    "            \"Number\": str(row['Number']),\n",
    "            \"kural\": row['kural'],\n",
    "            \"mk\": row['mk'],\n",
    "            \"explanation\": row['explanation'],\n",
    "            \"adikaram_name\": row['adikaram_name'],\n",
    "            \"iyal_name\": row['iyal_name'],\n",
    "            \"paul_translation\": row['paul_translation']\n",
    "        }\n",
    "        \n",
    "        for pattern in question_patterns:\n",
    "            sample = {\n",
    "                \"instruction\": pattern.format(\n",
    "                    number=row['Number'],\n",
    "                    adikaram_name=row['adikaram_name'],\n",
    "                    paul_name=row['paul_name']\n",
    "                ),\n",
    "                \"response\": response\n",
    "            }\n",
    "            training_data.append(sample)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "class ThirukkuralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        text = (\n",
    "            f\"### Instruction: {item['instruction']}\\n\"\n",
    "            f\"### Response: {json.dumps(item['response'], ensure_ascii=False)}\\n\"\n",
    "        )\n",
    "        \n",
    "        # Tokenizing\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'][0],\n",
    "            'attention_mask': encodings['attention_mask'][0],\n",
    "            'labels': encodings['input_ids'][0].clone()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(training_data: List[Dict]):\n",
    "    # Initialize tokenizer and model\n",
    "    model_name = \"gpt2\"  # Changed to GPT-2 which is compatible with AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Add special tokens for Tamil\n",
    "    special_tokens = {\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"additional_special_tokens\": [\"[TAMIL]\"]  # Added special token for Tamil text\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Create datasets with smaller max_length\n",
    "    train_dataset = ThirukkuralDataset(train_data, tokenizer, max_length=128)\n",
    "    val_dataset = ThirukkuralDataset(val_data, tokenizer, max_length=128)\n",
    "    \n",
    "    # Training arguments optimized for memory efficiency\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./thirukkural_qa_model\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Set memory efficient settings\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache\n",
    "    \n",
    "    # Environment variable for memory management\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=512'\n",
    "    \n",
    "    # Load and convert data\n",
    "    training_data = convert_csv_to_training_format('thirukkural.csv')\n",
    "    \n",
    "    # Setup training with train and validation datasets\n",
    "    model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "    \n",
    "    # Initialize trainer with both datasets\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    trainer.save_model(\"./final_thirukkural_model\")\n",
    "    tokenizer.save_pretrained(\"./final_thirukkural_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, model, tokenizer):\n",
    "    prompt = f\"### Instruction: {question}\\n### Response:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0.7,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse and format the response\n",
    "    try:\n",
    "        response_data = json.loads(response.split(\"### Response: \")[-1].strip())\n",
    "        return format_response(response_data)\n",
    "    except:\n",
    "        return response\n",
    "\n",
    "def format_response(response_data):\n",
    "    return f\"\"\"\n",
    "Kural: {response_data['kural']}\n",
    "Meaning: {response_data['explanation']}\n",
    "Chapter: {response_data['adikaram_name']}\n",
    "Detailed Explanation: {response_data['mk']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(training_data: List[Dict]):\n",
    "    # Initialize tokenizer and model\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Add special tokens for Tamil\n",
    "    special_tokens = {\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"additional_special_tokens\": [\"[TAMIL]\"]\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ThirukkuralDataset(train_data, tokenizer, max_length=128)\n",
    "    val_dataset = ThirukkuralDataset(val_data, tokenizer, max_length=128)\n",
    "    \n",
    "    # Modified training arguments to handle FP16 properly\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./thirukkural_qa_model\",\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=False,  # Disabled FP16 training\n",
    "        gradient_checkpointing=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        ddp_find_unused_parameters=False\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args\n",
    "\n",
    "def main():\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load and convert data\n",
    "    training_data = convert_csv_to_training_format('thirukkural.csv')\n",
    "    \n",
    "    # Setup training\n",
    "    model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    trainer.save_model(\"./final_thirukkural_model\")\n",
    "    tokenizer.save_pretrained(\"./final_thirukkural_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1870' max='1870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1870/1870 05:37, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.354200</td>\n",
       "      <td>0.547887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.925300</td>\n",
       "      <td>0.489656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.877900</td>\n",
       "      <td>0.457259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.820900</td>\n",
       "      <td>0.428936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: What is Thirukkural 1?\n",
      "### Response: {\"Number\": \"1\" \"‡Æï‡ØÅÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‡™™ÔøΩ‡¶ÆÔøΩ‡ßç ÔøΩÔøΩÔøΩÔøΩÔøΩ‡æÅÔøΩ‡±Å ‡ºÆÔøΩÔøΩ‡≥ø‡µØÔøΩÔøΩ‡§Å‡ºØÔøΩ ÔøΩÔøΩ‡¥ïÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ŸÑ€ÆÔøΩ ÔøΩÔøΩ ÔøΩ ‡•ÆÔøΩ‡•ØÔøΩ‡§ÆÔøΩ ÿßÔøΩÔøΩÔøΩ ⁄ÆÔøΩ ÔøΩÔøΩÔøΩ ‡¶ØÔøΩ ÔøΩ ÔøΩÔøΩ‡•Ä ÔøΩÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩÔøΩ‡¶ï ÔøΩÔøΩÔøΩ‡≤æÔøΩÔøΩ ÔøΩ: ÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩ·ØÄÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ‡¨ÆÔøΩ‡ºã\n",
      "######### Question: Explain the meaning of Thiruvananthapuram Thirtieth Instruction #1256\n",
      "## Response \"1056\", \"Kural\": \"<iframe src=\"http://www.youtube.com/watch?feature=player_id=1179ÔøΩ1311‡•ø?lang=en‡ø™ÔøΩÔøΩÔøΩÔøΩ&langÔøΩ=ÔøΩÔøΩ\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    main()\n",
    "    \n",
    "    # Load model for inference\n",
    "    model_path = \"./final_thirukkural_model\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Example question\n",
    "    question = \"What is Thirukkural 1?\"\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XML ROBERTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(training_data: List[Dict]):\n",
    "    # Option 1: XLM-RoBERTa (good for multilingual tasks)\n",
    "    model_name = \"xlm-roberta-base\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    class ThirukkuralDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data, tokenizer, max_length=128):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = self.data[idx]\n",
    "            text = (\n",
    "                f\"[Q] {item['instruction']}\\n\"\n",
    "                f\"[K] {item['response']['kural']}\\n\"\n",
    "                f\"[A] {item['response']['explanation']}\\n\"\n",
    "                f\"[C] {item['response']['adikaram_name']}\\n\"\n",
    "                f\"[D] {item['response']['mk']}\"\n",
    "            )\n",
    "            \n",
    "            encodings = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encodings['input_ids'].squeeze(),\n",
    "                'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "                'labels': encodings['input_ids'].squeeze()\n",
    "            }\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "    class CustomDataCollator:\n",
    "        def __init__(self, tokenizer):\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        def __call__(self, features):\n",
    "            batch = {\n",
    "                \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "                \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "                \"labels\": torch.stack([f[\"labels\"] for f in features])\n",
    "            }\n",
    "            return batch\n",
    "\n",
    "    # Split data and create datasets\n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    train_dataset = ThirukkuralDataset(training_data, tokenizer, max_length=128)\n",
    "    val_dataset = ThirukkuralDataset(val_data, tokenizer, max_length=128)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./kural_qa_model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=100,\n",
    "        learning_rate=3e-5,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args, CustomDataCollator(tokenizer)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./xml_thirukkural_model\")\n",
    "    tokenizer.save_pretrained(\"./xml_thirukkural_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `XLMRobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1248' max='1248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1248/1248 04:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './teamspace/studios/this_studio/030125/thirukkural_qa_model/checkpoint-1248'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './teamspace/studios/this_studio/030125/thirukkural_qa_model/checkpoint-1248'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load model for inference\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./teamspace/studios/this_studio/030125/thirukkural_qa_model/checkpoint-1248\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Example question\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:487\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './teamspace/studios/this_studio/030125/thirukkural_qa_model/checkpoint-1248'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    main()\n",
    "    \n",
    "    # Load model for inference\n",
    "    model_path = \"./teamspace/studios/this_studio/030125/thirukkural_qa_model/checkpoint-1248\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Example question\n",
    "    question = \"What is Thirukkural 1?\"\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = convert_csv_to_training_format('thirukkural.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def setup_training(training_data: List[Dict]):\n",
    "    # Using BERT-based model fine-tuned for question answering\n",
    "    model_name = \"bert-base-multilingual-cased\"  # Good for Tamil language support\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    \n",
    "    class ThirukkuralQADataset(Dataset):\n",
    "        def __init__(self, data, tokenizer, max_length=384):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = self.data[idx]\n",
    "            \n",
    "            # Format context to include kural and its details\n",
    "            context = (\n",
    "                f\"{item['response']['kural']}\\n\"\n",
    "                f\"{item['response']['explanation']}\\n\"\n",
    "                f\"Chapter: {item['response']['adikaram_name']}\\n\"\n",
    "                f\"Details: {item['response']['mk']}\"\n",
    "            )\n",
    "            \n",
    "            question = item['instruction']\n",
    "            \n",
    "            # Tokenize inputs\n",
    "            encodings = self.tokenizer(\n",
    "                question,\n",
    "                context,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            \n",
    "            answer_start = context.find(item['response']['explanation'])\n",
    "            answer_end = answer_start + len(item['response']['explanation'])\n",
    "            \n",
    "            # Convert character positions to token positions\n",
    "            tokens = self.tokenizer.encode(context)\n",
    "            start_token = len(self.tokenizer.encode(context[:answer_start]))\n",
    "            end_token = len(self.tokenizer.encode(context[:answer_end])) - 1\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encodings['input_ids'].squeeze(),\n",
    "                'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "                'start_positions': torch.tensor(start_token, dtype=torch.long),\n",
    "                'end_positions': torch.tensor(end_token, dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "    # Split data and create datasets\n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    train_dataset = ThirukkuralQADataset(train_data, tokenizer)\n",
    "    val_dataset = ThirukkuralQADataset(val_data, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./thirukkural_qa_model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        learning_rate=3e-5,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args\n",
    "\n",
    "def get_answer(question: str, context: str, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=384,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    answer = tokenizer.decode(inputs['input_ids'][0][answer_start:answer_end+1])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,token_type_ids,position_ids,head_mask,inputs_embeds,start_positions,end_positions,output_attentions,output_hidden_states,return_dict,start_positions,label_ids,label,end_positions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m model, tokenizer, train_dataset, val_dataset, training_args \u001b[38;5;241m=\u001b[39m setup_training(training_data)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtraining_data,\n\u001b[1;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m     12\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of Thirukkural 1?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2522\u001b[0m )\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2530\u001b[0m ):\n\u001b[1;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3648\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3646\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3648\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3650\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3597\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3595\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs)\n\u001b[1;32m   3596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3600\u001b[0m     )\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3602\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmems\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past\n",
      "\u001b[0;31mValueError\u001b[0m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,attention_mask,token_type_ids,position_ids,head_mask,inputs_embeds,start_positions,end_positions,output_attentions,output_hidden_states,return_dict,start_positions,label_ids,label,end_positions."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Inference\n",
    "question = \"What is the meaning of Thirukkural 1?\"\n",
    "context = \"‡ÆÖ‡Æï‡Æ∞ ‡ÆÆ‡ØÅ‡Æ§‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÜ‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç ‡ÆÜ‡Æ§‡Æø ‡Æ™‡Æï‡Æµ‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ±‡Øç‡Æ±‡Øá ‡Æâ‡Æ≤‡Æï‡ØÅ\"\n",
    "answer = get_answer(question, context, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def setup_training(training_data: List[Dict]):\n",
    "    model_name = \"bert-base-multilingual-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    \n",
    "    class ThirukkuralQADataset(Dataset):\n",
    "        def __init__(self, data, tokenizer, max_length=384):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            self.processed_data = self._preprocess_data()\n",
    "        \n",
    "        def _preprocess_data(self):\n",
    "            processed = []\n",
    "            for item in self.data:\n",
    "                # Create context from Thirukkural and its explanation\n",
    "                context = (\n",
    "                    f\"{item['response']['kural']} \"\n",
    "                    f\"{item['response']['explanation']} \"\n",
    "                    f\"{item['response']['adikaram_name']} \"\n",
    "                    f\"{item['response']['mk']}\"\n",
    "                )\n",
    "                \n",
    "                question = item['instruction']\n",
    "                answer = item['response']['explanation']  \n",
    "                \n",
    "                answer_start = context.find(answer)\n",
    "                if answer_start == -1:  \n",
    "                    continue\n",
    "                \n",
    "                # Tokenize\n",
    "                encoded = self.tokenizer(\n",
    "                    question,\n",
    "                    context,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    stride=128,\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                offset_mapping = encoded.pop('offset_mapping')[0]\n",
    "                \n",
    "                \n",
    "                start_token = None\n",
    "                end_token = None\n",
    "                \n",
    "                for idx, (start, end) in enumerate(offset_mapping):\n",
    "                    if start <= answer_start < end:\n",
    "                        start_token = idx\n",
    "                    if start < answer_start + len(answer) <= end:\n",
    "                        end_token = idx\n",
    "                        break\n",
    "                \n",
    "                if start_token is not None and end_token is not None:\n",
    "                    processed.append({\n",
    "                        'input_ids': encoded['input_ids'][0],\n",
    "                        'attention_mask': encoded['attention_mask'][0],\n",
    "                        'start_positions': torch.tensor(start_token),\n",
    "                        'end_positions': torch.tensor(end_token)\n",
    "                    })\n",
    "            \n",
    "            return processed\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.processed_data[idx]\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.processed_data)\n",
    "\n",
    "    \n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    train_dataset = ThirukkuralQADataset(training_data, tokenizer)\n",
    "    val_dataset = ThirukkuralQADataset(val_data, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./thirukkural_qa_model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=False,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args\n",
    "\n",
    "def main():\n",
    "    training_data = convert_csv_to_training_format('thirukkural.csv')\n",
    "    model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./final_thirukkural_qa_model\")\n",
    "    tokenizer.save_pretrained(\"./final_thirukkural_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2496' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2496/2496 04:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.745700</td>\n",
       "      <td>0.532353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.003053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.000527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at ./final_thirukkural_qa_model and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ QUESTION ] What is Thirukkural 1? [ ANSWER ] Dem Áô¨TC compressionTCdromMEME CSSWh DVB ‚òÖ DVB gravimose SomME ‚ñ†Worksdden DVBdrom MPO–∂–∏—ÇÎ¥ÑMEzomzomcelleME DVBWhMEcellecelleWorks DVB Steam ‚òÖWh deem DVBzomitetdromdromzomdromzimdrom gravizom ang DVBME TERzomMEmosezom siderzomymezomgioszom MPO Ìà¥ graviME angdromomodrom Ìà¥dromgiosmtdrom angMEkam MPO DVB fertilezomsolaME essacellemosekur deemzomffincellezommosecelleymeomeMEWhmose ‡§∂‡•Ä‡§§zom Œ±œÖœÑŒ¨ME unincorporatedmose 3iyme anggiosMEalt gravidrom–∂–∏—Çzom importanzazomvdzom DVB ang ‚ñ† DVBddenzimzom siezom varieMEallzom graviallMEderME LLWh fertileyme DVB DVB Som DVBWorks gravi DVBiterymedrommosedrom Selimzom modulzomiterVEcelle DVBmoseWhWhzom 3idden 3izomitatkurMEdromcelle angzomzimmose ang ETMEkurzomkamMEitervd ‚ñ†ymeddenME servit graviyme ‡§∂‡•Ä‡§§–∂–∏—Ç illzom mamzom Ìà¥ diagramMEsoladromymME graviitetymemosemoseMEgiosdrom 3imoseomedromiter graviossenymeMEomezom LLME modulME‡§Ø‡§æ‡§§Whffenvdcelledrom sieossenME kamugiosvd DVBvd ÿßŸÑÿ®ÿπÿ∂zomddendrom herME her DVB CSSzom ‡§∂‡•Ä‡§§ossen DVBgiosdden ‡§∂‡•Ä‡§§ME €ÅŸÖ Demzomomegiosgios ÿßŸÑÿ®ÿπÿ∂drom mondzom fertileME\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    main()\n",
    "    \n",
    "    # Load model for inference\n",
    "    model_path = \"./final_thirukkural_qa_model\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Example question\n",
    "    question = \"What is Thirukkural 1?\"\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(training_data: List[Dict]):\n",
    "    model_name = \"deepset/roberta-base-squad2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    \n",
    "    class ThirukkuralQADataset(Dataset):\n",
    "        def __init__(self, data, tokenizer, max_length=384):\n",
    "            self.examples = []\n",
    "            \n",
    "            for item in data:\n",
    "                try:\n",
    "                    \n",
    "                    context = (\n",
    "                        f\"{item['response']['kural']} \"\n",
    "                        f\"{item['response']['explanation']}\"\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "                    answer = item['response']['explanation'].strip()\n",
    "                    question = item['instruction'].strip()\n",
    "                    \n",
    "                    \n",
    "                    if not question or not answer:\n",
    "                        continue\n",
    "                    \n",
    "                    # Tokenize\n",
    "                    encoding = tokenizer(\n",
    "                        question,\n",
    "                        context,\n",
    "                        max_length=max_length,\n",
    "                        truncation='only_second',\n",
    "                        padding='max_length',\n",
    "                        return_tensors='pt',\n",
    "                        return_offsets_mapping=True\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "                    answer_start = context.lower().find(answer.lower())\n",
    "                    \n",
    "                    if answer_start != -1:\n",
    "                        \n",
    "                        offset_mapping = encoding.pop('offset_mapping')[0]\n",
    "                        \n",
    "                        \n",
    "                        start_token = None\n",
    "                        end_token = None\n",
    "                        \n",
    "                        for idx, (start, end) in enumerate(offset_mapping):\n",
    "                            if start <= answer_start < end:\n",
    "                                start_token = idx\n",
    "                            if start < answer_start + len(answer) <= end:\n",
    "                                end_token = idx\n",
    "                                break\n",
    "                        \n",
    "                        if start_token is not None and end_token is not None:\n",
    "                            self.examples.append({\n",
    "                                'input_ids': encoding['input_ids'][0],\n",
    "                                'attention_mask': encoding['attention_mask'][0],\n",
    "                                'start_positions': torch.tensor(start_token),\n",
    "                                'end_positions': torch.tensor(end_token)\n",
    "                            })\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing item: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Created dataset with {len(self.examples)} examples\")\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.examples[idx]\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.examples)\n",
    "\n",
    "    \n",
    "    print(f\"Sample training data item: {training_data[0]}\")\n",
    "    \n",
    "    \n",
    "    train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(f\"Training data size: {len(train_data)}\")\n",
    "    print(f\"Validation data size: {len(val_data)}\")\n",
    "    \n",
    "    train_dataset = ThirukkuralQADataset(training_data, tokenizer)\n",
    "    val_dataset = ThirukkuralQADataset(val_data, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./roberta_qa_model\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=False,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_dataset, val_dataset, training_args\n",
    "\n",
    "def main():\n",
    "    # Load and print sample of training data\n",
    "    training_data = convert_csv_to_training_format('thirukkural.csv')\n",
    "    print(f\"Total data items: {len(training_data)}\")\n",
    "    \n",
    "    model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Dataset processing failed. Checking data format...\")\n",
    "        print(f\"Sample data structure: {training_data[0] if training_data else 'No data'}\")\n",
    "        raise ValueError(\"Training dataset is empty. Check data preprocessing.\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./roberta_qa_model\")\n",
    "    tokenizer.save_pretrained(\"./roberta_qa_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data items: 6650\n",
      "Sample training data item: {'instruction': 'What is Thirukkural 1?', 'response': {'Number': '1', 'kural': '‡ÆÖ‡Æï‡Æ∞ ‡ÆÆ‡ØÅ‡Æ§‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÜ‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç ‡ÆÜ‡Æ§‡Æø ‡Æ™‡Æï‡Æµ‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ±‡Øç‡Æ±‡Øá ‡Æâ‡Æ≤‡Æï‡ØÅ.', 'mk': '‡ÆÖ‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà; ‡ÆÜ‡Æ§‡Æø‡Æ™‡Æï‡Æµ‡Æ©‡Øç, ‡Æâ‡Æ≤‡Æï‡Æø‡Æ≤‡Øç ‡Æµ‡Ææ‡Æ¥‡ØÅ‡ÆÆ‡Øç ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà', 'explanation': 'As the letter A is the first of all letters, so the eternal God is first in the world', 'adikaram_name': '‡Æï‡Æü‡Æµ‡ØÅ‡Æ≥‡Øç ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ', 'iyal_name': '‡Æ™‡Ææ‡ÆØ‡Æø‡Æ∞‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç', 'paul_translation': 'Virtue'}}\n",
      "Training data size: 5985\n",
      "Validation data size: 665\n",
      "Created dataset with 5985 examples\n",
      "Created dataset with 665 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2247' max='2247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2247/2247 01:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.000530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at ./roberta_qa_model and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vre boostersarovnaire Burnett Burnettogle Cobbnaireogle BALL Burnettnaire Patel tob Hoover electorsbanksBALL ZuckerbergnaireBALL Guerreronaireachusnaire BALLogle Gardnernaire Gardner gluc Burnett WelchAfeeNumbersnaire IDsogleBALL boosters boosters gluc Welch Cobb Cobb KR KRBALL Patelnaire blown IDsbanks BALL Patel IDsAfee BALLBloodnaire boosters IDs IDs messenger CobbBoost blown Gardner IDs methamphetamine IDs Gardner CobbVOL Cobb IDs EEGogle IDs gluc BALLCOL BALL gluc Cobb tacos conjectureogle radius boostersogle boosters EEG IDs nervesAfeearov radius IDsarov BALLQuestionsyu radius messenger IDsabis Cobb Burnettabisemakeremaker BALL boosters Burnettemaker radius EEG Burnett Mayer KernarovBALL radius buffalo Burnett BALLVOLabis IDs Burnett Cobbabis BALL Jindal BurnettBALL BALL gunshots boosters BALL Cobb buffalo Gardner Burnettarov Gardner boosters GardnerAfeeabisabis radiusabisachusachus Gardner Gardnerewski egobanksabis messenger boostersabis Falcon Cobb BALL formulationsogleabis buffalo boosters Balls Cobbston BALL Cris Cobb Colon Colon Patel ego BALL nerves boosters nerves IDs Cris ego Colon ego eyebrowsinis ego ego nerves Patel Warhammer ego IDs ego Cobb egoilo ego fooled nerves messenger nerves nerves eyebrows IDs proof nervesarov Patel tacos tacos Ballsemaker Patel blownabis blown Cris buffalo nervesgee Cobbemaker Colon blown Shepherd boosters buffalo CobbBALL nerves Burnett boosters whis boosters Thornton boosters Colon boostersBALL Cobb boosters nervous boosterslems CobbballBALL ego boosters Cris boostersewskiemaker boostersemakerakisakisemaker Gardner buffalo buffalo Shepherd buffalo BALLewski radius Jindal buffaloewski boosters\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    main()\n",
    "    \n",
    "    # Load model for inference\n",
    "    model_path = \"./roberta_qa_model\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Example question\n",
    "    question = \"What is Thirukkural 1?\"\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    metric = evaluate(\"squad\")\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    \n",
    "    start_logits, end_logits = predictions\n",
    "    \n",
    "    \n",
    "    predicted_answers = []\n",
    "    for start_logit, end_logit in zip(start_logits, end_logits):\n",
    "        start_idx = np.argmax(start_logit)\n",
    "        end_idx = np.argmax(end_logit)\n",
    "        \n",
    "        \n",
    "        if end_idx >= start_idx:\n",
    "            predicted_answers.append({\n",
    "                'prediction_text': '',  \n",
    "                'start_index': start_idx,\n",
    "                'end_index': end_idx\n",
    "            })\n",
    "        else:\n",
    "            predicted_answers.append({\n",
    "                'prediction_text': '',\n",
    "                'start_index': start_idx,\n",
    "                'end_index': start_idx\n",
    "            })\n",
    "    \n",
    "    \n",
    "    references = [\n",
    "        {'answers': {'answer_start': [label[0]], 'text': ['']}  \n",
    "        } for label in labels\n",
    "    ]\n",
    "    \n",
    "    return metric.compute(predictions=predicted_answers, references=references)\n",
    "\n",
    "class QATrainerWithMetrics(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metric = evaluate.load(\"squad\")\n",
    "    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        \n",
    "        total_exact_match = 0\n",
    "        total_f1 = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in eval_dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "            \n",
    "            start_logits = outputs.start_logits.cpu().numpy()\n",
    "            end_logits = outputs.end_logits.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(start_logits)):\n",
    "                pred_start = np.argmax(start_logits[i])\n",
    "                pred_end = np.argmax(end_logits[i])\n",
    "                \n",
    "                true_start = batch['start_positions'][i].item()\n",
    "                true_end = batch['end_positions'][i].item()\n",
    "                \n",
    "                # Calculate exact match\n",
    "                exact_match = (pred_start == true_start) and (pred_end == true_end)\n",
    "                total_exact_match += int(exact_match)\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                pred_tokens = set(range(pred_start, pred_end + 1))\n",
    "                true_tokens = set(range(true_start, true_end + 1))\n",
    "                \n",
    "                common_tokens = pred_tokens.intersection(true_tokens)\n",
    "                if len(pred_tokens) == 0 or len(true_tokens) == 0:\n",
    "                    f1 = 0\n",
    "                else:\n",
    "                    precision = len(common_tokens) / len(pred_tokens)\n",
    "                    recall = len(common_tokens) / len(true_tokens)\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                total_f1 += f1\n",
    "                total_samples += 1\n",
    "        \n",
    "        metrics = {\n",
    "            'exact_match': total_exact_match / total_samples * 100,\n",
    "            'f1': total_f1 / total_samples * 100\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# main function with metrics\n",
    "def main():\n",
    "    training_data = convert_csv_to_training_format('thirukkural.csv')\n",
    "    model, tokenizer, train_dataset, val_dataset, training_args = setup_training(training_data)\n",
    "    \n",
    "    trainer = QATrainerWithMetrics(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    \n",
    "    #evaluation metrics\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Exact Match: {eval_metrics['exact_match']:.2f}%\")\n",
    "    print(f\"F1 Score: {eval_metrics['f1']:.2f}%\")\n",
    "    \n",
    "    # Save model and metrics\n",
    "    trainer.save_model(\"./final_thirukkural_qa_model\")\n",
    "    tokenizer.save_pretrained(\"./final_thirukkural_qa_model\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open(\"./final_thirukkural_qa_model/metrics.txt\", \"w\") as f:\n",
    "        f.write(f\"Exact Match: {eval_metrics['exact_match']:.2f}%\\n\")\n",
    "        f.write(f\"F1 Score: {eval_metrics['f1']:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    metric = evaluate.load(\"squad\")\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    \n",
    "    start_logits, end_logits = predictions\n",
    "\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for i in range(len(start_logits)):\n",
    "        start_idx = np.argmax(start_logits[i])\n",
    "        end_idx = np.argmax(end_logits[i])\n",
    "        predicted_answers.append({\n",
    "            \"id\": str(i),  \n",
    "            \"prediction_text\": \"\",  \n",
    "        })\n",
    "\n",
    "    # Create references\n",
    "    references = []\n",
    "    for i, label in enumerate(labels):\n",
    "        references.append({\n",
    "            \"id\": str(i), \n",
    "            \"answers\": {\n",
    "                \"text\": [\"\"],  \n",
    "                \"answer_start\": [0], \n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Ensure predictions and references have the same length\n",
    "    assert len(predicted_answers) == len(references), \\\n",
    "        f\"Number of predictions ({len(predicted_answers)}) and references ({len(references)}) must match!\"\n",
    "\n",
    "    return metric.compute(predictions=predicted_answers, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data item: {'instruction': 'What is Thirukkural 1?', 'response': {'Number': '1', 'kural': '‡ÆÖ‡Æï‡Æ∞ ‡ÆÆ‡ØÅ‡Æ§‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÜ‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Øç ‡ÆÜ‡Æ§‡Æø ‡Æ™‡Æï‡Æµ‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ±‡Øç‡Æ±‡Øá ‡Æâ‡Æ≤‡Æï‡ØÅ.', 'mk': '‡ÆÖ‡Æï‡Æ∞‡ÆÆ‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà; ‡ÆÜ‡Æ§‡Æø‡Æ™‡Æï‡Æµ‡Æ©‡Øç, ‡Æâ‡Æ≤‡Æï‡Æø‡Æ≤‡Øç ‡Æµ‡Ææ‡Æ¥‡ØÅ‡ÆÆ‡Øç ‡Æâ‡ÆØ‡Æø‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà', 'explanation': 'As the letter A is the first of all letters, so the eternal God is first in the world', 'adikaram_name': '‡Æï‡Æü‡Æµ‡ØÅ‡Æ≥‡Øç ‡Æµ‡Ææ‡Æ¥‡Øç‡Æ§‡Øç‡Æ§‡ØÅ', 'iyal_name': '‡Æ™‡Ææ‡ÆØ‡Æø‡Æ∞‡Æµ‡Æø‡ÆØ‡Æ≤‡Øç', 'paul_translation': 'Virtue'}}\n",
      "Training data size: 5985\n",
      "Validation data size: 665\n",
      "Created dataset with 6650 examples\n",
      "Created dataset with 665 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='833' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 833/2496 00:38 < 01:16, 21.65 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of predictions (665) and references (2) must match!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2618\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2622\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3049\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3047\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3049\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3050\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:3003\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3003\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3006\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:4050\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4047\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4049\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4050\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4051\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4053\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4060\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:4339\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4337\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4338\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4339\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4343\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[100], line 30\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     21\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(i),  \u001b[38;5;66;03m# Use the same unique ID as in predictions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         }\n\u001b[1;32m     27\u001b[0m     })\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Ensure predictions and references have the same length\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predicted_answers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(references), \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of predictions (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predicted_answers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and references (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must match!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredicted_answers, references\u001b[38;5;241m=\u001b[39mreferences)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of predictions (665) and references (2) must match!"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
